{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "from annotated_text import annotated_text, parameters\n",
    "from streamlit_extras import add_vertical_space as avs\n",
    "from streamlit_extras.badges import badge\n",
    "from scripts.similarity import get_similarity_score, find_path, read_config\n",
    "from scripts.utils import get_filenames_from_dir\n",
    "from scripts import ReadPdf, JobDescriptionProcessor, ResumeProcessor, KeytermsExtraction\n",
    "import cohere\n",
    "from scripts.KeytermsExtraction import KeytermExtractor\n",
    "from scripts.similarity.get_similarity_score import get_similarity_score\n",
    "import uuid\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='app_similarity_score.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "console_handler.setFormatter(formatter)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "file_handler = logging.FileHandler(\"app_similarity_score.log\")\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "cwd = find_path('Resume-Matcher')\n",
    "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
    "\n",
    "\n",
    "def read_config(filepath):\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        return config\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        logger.error(\n",
    "            f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "config = read_config(config_path + \"/config.yml\")\n",
    "PROJECT_ID = config['vertex']['api_key']\n",
    "REGION = config['vertex']['REGION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_STACK = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "job_desc_directory = \"Data/JobDescription/\"\n",
    "resumes_directory = \"Data/Resumes/\"\n",
    "\n",
    "# Automatically get all job description and resume files\n",
    "job_desc_files = [file for file in os.listdir(\n",
    "    job_desc_directory) if file.endswith('.pdf')]\n",
    "resume_files = [file for file in os.listdir(\n",
    "    resumes_directory) if file.endswith('.pdf')]\n",
    "\n",
    "# This will select the first job description file\n",
    "job_desc_file = job_desc_files[FULL_STACK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = \"mongodb+srv://chl155:3DiufR0lg70F9s4P@cluster0.bkmn6rk.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose or create a database named 'resume_db'\n",
    "db = client['resume_db']\n",
    "\n",
    "# Choose or create a collection named 'candidates'\n",
    "candidates_collection = db['candidates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose or create a database named 'job_db'\n",
    "db = client['job_db']\n",
    "job_collection = db['job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# Read raw job description\n",
    "# job_desc_text = ReadPdf.read_single_pdf(\"Data/JobDescription/\" + job_desc_file)\n",
    "job_desc_text = [ReadPdf.read_single_pdf(os.path.join(\n",
    "    \"Data/JobDescription/\", job_desc_file)) for job_desc_file in job_desc_files]\n",
    "# Process job description\n",
    "job_desc_processor = JobDescriptionProcessor(job_desc_file)\n",
    "job_desc_processed = job_desc_processor._read_job_desc()\n",
    "\n",
    "job_files = [f for f in os.listdir(\n",
    "    \"Data/JobDescription/\") if os.path.isfile(os.path.join(\"Data/JobDescription/\", f))]\n",
    "\n",
    "\n",
    "job_processed = []\n",
    "for job_file in job_files:\n",
    "    job_processor = JobDescriptionProcessor(job_file)\n",
    "    job_data = job_processor._read_job_desc()\n",
    "    job_processor._write_json_file(job_data)\n",
    "    job_processed.append(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resumes_text = [ReadPdf.read_single_pdf(os.path.join(\n",
    "    \"Data/Resumes/\", resume_file)) for resume_file in resume_files]\n",
    "\n",
    "\n",
    "# Process resumes\n",
    "resume_files = [f for f in os.listdir(\n",
    "    \"Data/Resumes/\") if os.path.isfile(os.path.join(\"Data/Resumes/\", f))]\n",
    "\n",
    "resumes_processed = []\n",
    "for resume_file in resume_files:\n",
    "    resume_processor = ResumeProcessor(resume_file)\n",
    "    resume_data = resume_processor._read_resumes()\n",
    "    resume_processor._write_json_file(resume_data)\n",
    "    resumes_processed.append(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_name_from_filename(filename: str) -> str:\n",
    "    # Exclude the last part which is the position\n",
    "    name_parts = filename.split('_')[:-1]\n",
    "    return ' '.join(name_parts).title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yiching Liu',\n",
       " 'Angela Zhu',\n",
       " 'Maria Chinkan',\n",
       " 'Arpi Melik Parsadanyan',\n",
       " 'Zihui Lin',\n",
       " 'Jose Felix Villasenor',\n",
       " 'Anna Gasparyan',\n",
       " 'Amitesh Rathore',\n",
       " 'Yunrui Shao',\n",
       " 'Jaykumar',\n",
       " 'Minyue Yao',\n",
       " 'John',\n",
       " 'Zane Rouguine',\n",
       " 'Brandon Penner',\n",
       " 'Zhe Wang',\n",
       " 'Tsubasa Lin',\n",
       " 'Danny Mai',\n",
       " 'David Boutwell',\n",
       " 'Alexandra',\n",
       " 'Sarah Sherman',\n",
       " 'David Botbol',\n",
       " 'Ryan Pintar',\n",
       " 'Eloise Yu',\n",
       " 'Vasil Klimovich',\n",
       " 'Ming Jin',\n",
       " 'Robert Scozzari',\n",
       " 'Cody Romero',\n",
       " 'Carnell Brame',\n",
       " 'Timothy Wang',\n",
       " 'Nico Santoso',\n",
       " 'Shirley Zhao',\n",
       " 'Yuan Wang',\n",
       " 'Divya Harshini',\n",
       " 'Deekshitha Pullaiah',\n",
       " 'Meredith Cheng',\n",
       " 'Grace Li',\n",
       " 'Anya Hsu',\n",
       " 'Nandini Seth',\n",
       " 'Andrew Knuppel',\n",
       " 'John Hinnegan',\n",
       " 'Sharad Dangol',\n",
       " 'Salvador Campos',\n",
       " 'Lauren Aubrey Lee',\n",
       " 'Xiao Li',\n",
       " 'Bruce Wayne',\n",
       " 'Barry Allen',\n",
       " 'Balraj Rai',\n",
       " 'Annie Zhou',\n",
       " 'Federico De Marines',\n",
       " 'Ray Lee',\n",
       " 'Jagriti Sharma',\n",
       " 'Michelle Wang',\n",
       " 'Galen Fink',\n",
       " 'Dennis Mo',\n",
       " 'Mengyao Zhang',\n",
       " 'Johann C',\n",
       " 'Yixin-Ying',\n",
       " 'Peggy Lai',\n",
       " 'Yuanhuang Lo',\n",
       " 'Serleen Lee',\n",
       " 'Maria Gonzalez',\n",
       " 'Natalie Le',\n",
       " 'Yuying Ma',\n",
       " 'Aramis Nia',\n",
       " 'Diego Arcadia',\n",
       " 'Alexander Gasca',\n",
       " 'Rulin Xing',\n",
       " 'Xiaoda Li',\n",
       " 'Haley Carruthers',\n",
       " 'Yunjae Kim',\n",
       " 'Nuocheng Peng',\n",
       " 'Yihao Xu',\n",
       " 'Mckenna Bass',\n",
       " 'John Zhou',\n",
       " 'Steven Meneses',\n",
       " 'Harvey Dent',\n",
       " 'Arif Demirkan',\n",
       " 'Austin Zuo',\n",
       " 'Jiayu Yi',\n",
       " 'Lisa Udechukwu',\n",
       " 'Dennis Tou',\n",
       " 'Adrian Velasco',\n",
       " 'Hanfei He',\n",
       " 'Sheen Huang',\n",
       " 'Xinyi Yu',\n",
       " 'Alfred Pennyworth',\n",
       " 'Chenjie Wu',\n",
       " 'Yaxing Li',\n",
       " 'Jeffrey Chen',\n",
       " 'Yiwen Ding',\n",
       " 'Steven Chen',\n",
       " 'Charles Zhang',\n",
       " 'Polina Popova']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_names = [extract_candidate_name_from_filename(\n",
    "    resume_file) for resume_file in resume_files]\n",
    "candidate_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumes_processed[0]['name']\n",
    "\n",
    "for i in range(len(resumes_processed)):\n",
    "    resumes_processed[i]['name'] = candidate_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x2fed1a020>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_collection.insert_many(resumes_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobdes_name_from_filename(filename: str) -> str:\n",
    "    # Exclude the last part which is the position\n",
    "    name_parts = filename.split('_')[:-1]\n",
    "    return ' '.join(name_parts).title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des = [extract_jobdes_name_from_filename(\n",
    "    job_desc_file) for job_desc_file in job_desc_files]\n",
    "job_des = [item.replace('Job Desc ', '') for item in job_des]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(job_processed)):\n",
    "    job_processed[i].update({'job_title': job_des[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x31e532890>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_collection.insert_many(job_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des_parse_data = job_collection.find({}, {'unique_id':1,'clean_data':1,'extracted_keywords':1,'_id':0,'job_title':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des_parse_data = list(job_des_parse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_to_index = {}\n",
    "job_unique_id = []\n",
    "\n",
    "for index, data in enumerate(job_des_parse_data):\n",
    "    # Convert the job_title to uppercase and replace spaces with underscores\n",
    "    constant_name = data['job_title'].upper().replace(' ', '_')\n",
    "    job_unique_id.append(data['unique_id'])\n",
    "    # Set the constant name as a key in the dictionary with its index as the value\n",
    "    job_title_to_index[constant_name] = index\n",
    "\n",
    "# To access a particular index:\n",
    "# index_for_full_stack = job_title_to_index['FULL_STACK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the candidates from the MongoDB collection\n",
    "candidates_parse_data = candidates_collection.find(\n",
    "    {}, {\"name\": 1, \"unique_id\": 1, \"_id\": 0, \"clean_data\": 1, \"extracted_keywords\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_parse_data = list(candidates_parse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_parse_data_holder = candidates_parse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_unique_id = []\n",
    "\n",
    "for index, data in enumerate(candidates_parse_data):\n",
    "    # Convert the job_title to uppercase and replace spaces with underscores\n",
    "    candidates_unique_id.append(data['unique_id'])\n",
    "    # Set the constant name as a key in the dictionary with its index as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list for resumes_keywords if it doesn't exist yet\n",
    "\n",
    "\n",
    "# Iterate over the cursor and update resumes_keywords\n",
    "# Initialize an empty list for resumes_keywords if it doesn't exist yet\n",
    "# Initialize an empty list for resumes_keywords if it doesn't exist yet\n",
    "resumes_keywords_str_list = []\n",
    "\n",
    "# Iterate over the cursor and update resumes_keywords\n",
    "for document in candidates_parse_data_holder:\n",
    "    keyword_string = ' '.join(document['extracted_keywords'])\n",
    "    resumes_keywords_str_list.append(keyword_string)\n",
    "    # resumes_keywords.append(document['extracted_keywords'])\n",
    "\n",
    "\n",
    "# for index in range(len(candidates_parse_data)):\n",
    "#     candidates_parse_data[index]['extracted_keywords'] = resumes_keywords[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_clean_data_str_list = []\n",
    "\n",
    "# Iterate over the cursor and update resumes_keywords\n",
    "for document in candidates_parse_data:\n",
    "    keyword_string = ' '.join(document['clean_data'])\n",
    "    resumes_clean_data_str_list.append(keyword_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the extracted keywords into strings\n",
    "# job_desc_keywords_str = ' '.join([keyword[0] for keyword in job_desc_keywords])\n",
    "job_desc_keywords_str_list = []\n",
    "for index in range(len(job_des_parse_data)):\n",
    "    keyword_string = ' '.join(document['extracted_keywords'])\n",
    "    job_desc_keywords_str_list.append(keyword_string)\n",
    "    #job_desc_keywords_str_list.append(job_des_parse_data[index]['clean_data'])\n",
    "    #job_desc_keywords_str_list.append(job_des_parse_data[index]['extracted_keywords'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_names = [extract_candidate_name_from_filename(\n",
    "    resume_file) for resume_file in resume_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#job_desc_embeddings = embeddings.embed_documents(job_desc_keywords_str_list)\n",
    "# Assuming job_desc_keywords_str_list is a list of lists\n",
    "# flattened_list = [\n",
    "#     item for sublist in job_desc_keywords_str_list for item in sublist]\n",
    "\n",
    "# Now, each element of flattened_list should be a string\n",
    "job_desc_embeddings = embeddings.embed_documents(job_desc_keywords_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:15<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "resumes_embeddings = embeddings.embed_documents(resumes_keywords_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "\tapi_key='75a74122-c7d4-470d-a892-df788c806fb7',\n",
    "\tenvironment='gcp-starter'\n",
    ")\n",
    "\n",
    "index = pinecone.Index(index_name=\"jobmatcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_for_full_stack = job_title_to_index['FULL_STACK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des_underscore = [name.replace(' ', '_') for name in job_des]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_names_underscore = [name.replace(\n",
    "    ' ', '_') for name in candidate_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 12\n"
     ]
    }
   ],
   "source": [
    "print(len(job_desc_embeddings), len(job_des_underscore), len(job_unique_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb Cell 40\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m job_vectors_with_metadata \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, embedding \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(job_desc_embeddings):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     vector_data \u001b[39m=\u001b[39m {\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: job_des_underscore[i],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m: embedding,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39munique_id\u001b[39m\u001b[39m\"\u001b[39m: job_unique_id[i]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     job_vectors_with_metadata\u001b[39m.\u001b[39mappend(vector_data)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "job_vectors_with_metadata = []\n",
    "\n",
    "for i, embedding in enumerate(job_desc_embeddings):\n",
    "    vector_data = {\n",
    "        \"id\": job_des_underscore[i],\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": {\"unique_id\": job_unique_id[i]}\n",
    "    }\n",
    "    job_vectors_with_metadata.append(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb Cell 40\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m candidates_vectors_with_metadata \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, embedding \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(resumes_embeddings):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     vector_data \u001b[39m=\u001b[39m {\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: candidate_names_underscore[i],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m: embedding,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39munique_id\u001b[39m\u001b[39m\"\u001b[39m: candidates_unique_id[i]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liuchuning/USC/Intellipro/Resume-Matcher/Job_Match_longchain.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     candidates_vectors_with_metadata\u001b[39m.\u001b[39mappend(vector_data)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "candidates_vectors_with_metadata = []\n",
    "\n",
    "for i, embedding in enumerate(resumes_embeddings):\n",
    "    vector_data = {\n",
    "        \"id\": candidate_names_underscore[i],\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": {\"unique_id\": candidates_unique_id[i]}\n",
    "    }\n",
    "    candidates_vectors_with_metadata.append(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors_with_metadata = job_vectors_with_metadata + \\\n",
    "    candidates_vectors_with_metadata\n",
    "index.upsert(vectors=all_vectors_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_fetched_vectors = index.fetch(ids=candidate_names_underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_fetched_vectors = index.fetch(ids=job_des_underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_job_vectors = {}\n",
    "\n",
    "for name, data in jobs_fetched_vectors['vectors'].items():\n",
    "    all_job_vectors[name] = data['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidate_vectors = {}\n",
    "\n",
    "for name, data in candidates_fetched_vectors['vectors'].items():\n",
    "    all_candidate_vectors[name] = data['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_job_vectors['Full_Stack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cosine_similarity(embedding1: List[float], embedding2: List[float]) -> float:\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarities_with_names = []\n",
    "\n",
    "# Assuming all_job_vectors['Full_Stack'] gives you the vector for the 'Full_Stack' job.\n",
    "full_stack_vector = all_job_vectors['Full_Stack']\n",
    "\n",
    "for candidate_name, candidate_vector in all_candidate_vectors.items():\n",
    "    similarity_score = cosine_similarity(\n",
    "        [full_stack_vector], [candidate_vector])[0][0]\n",
    "    similarities_with_names.append({\n",
    "        \"name\": candidate_name,\n",
    "        \"similarity\": similarity_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_candidates = sorted(similarities_with_names,\n",
    "                           key=lambda x: x[\"similarity\"], reverse=True)\n",
    "\n",
    "# Print the ranked candidates\n",
    "# Starting the index from 1\n",
    "for index, candidate in enumerate(ranked_candidates, 1):\n",
    "    print(\n",
    "        f\"Candidate {index}: {candidate['name']}, Similarity Score: {candidate['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Set the file path\n",
    "file_path = 'Data/Result/ranked_candidates_hugging.csv'\n",
    "\n",
    "# Prepare the data for CSV\n",
    "rows = [[\"Rank\", \"Candidate Name\", \"Similarity Score\"]]\n",
    "for index, candidate in enumerate(ranked_candidates, 1):\n",
    "    rows.append([index, candidate['name'], candidate['similarity']])\n",
    "\n",
    "# Save to CSV\n",
    "with open(file_path, \"w\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
